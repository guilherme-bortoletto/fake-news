# Detec√ß√£o de Fake News com Processamento em Larga Escala

**Autores:**

* Vinicius Silva Castro ‚Äì 802138

* Guilherme Campos Bortoletto ‚Äì 801477

**Ano:** 2025

---

## üìÑ Resumo

Este projeto desenvolve um pipeline completo para a detec√ß√£o autom√°tica de *fake news*, integrando tecnologias de armazenamento NoSQL e processamento de dados em larga escala. Utilizando o **MongoDB Atlas** como banco de dados e o **Apache Spark (PySpark)** como plataforma de processamento distribu√≠do, o sistema √© capaz de armazenar, transformar e analisar um grande volume de not√≠cias jornal√≠sticas.

A solu√ß√£o utiliza um dataset do Kaggle com aproximadamente 44.000 not√≠cias, que s√£o pr√©-processadas e inseridas no MongoDB. Em seguida, o pipeline com Spark executa tarefas de Processamento de Linguagem Natural (NLP), como tokeniza√ß√£o e vetoriza√ß√£o com TF-IDF, para treinar modelos de *machine learning* capazes de classificar textos como verdadeiros ou falsos. Al√©m da classifica√ß√£o, o projeto inclui uma an√°lise explorat√≥ria para identificar padr√µes lingu√≠sticos, como as palavras mais frequentes em conte√∫dos falsos, oferecendo insights sobre o vocabul√°rio da desinforma√ß√£o.

## üéØ Objetivo

O objetivo principal √© construir um pipeline de ponta a ponta para a detec√ß√£o de not√≠cias falsas, demonstrando a aplica√ß√£o de tecnologias modernas para:

* **Armazenar** de forma flex√≠vel e escal√°vel um grande volume de not√≠cias reais e falsas.

* **Processar** os dados textuais em larga escala com t√©cnicas de NLP.

* **Treinar e avaliar** um modelo de *machine learning* para classifica√ß√£o autom√°tica de not√≠cias.

* **Avaliar a viabilidade** da arquitetura proposta (MongoDB + Spark) em um contexto de aplica√ß√£o real que demanda escalabilidade e robustez.

* **Identificar padr√µes lingu√≠sticos** em textos de *fake news* atrav√©s de an√°lises explorat√≥rias.

## üõ†Ô∏è Tecnologias Escolhidas

### MongoDB Atlas

A escolha de um banco de dados NoSQL como o **MongoDB** foi fundamental devido √† natureza semi-estruturada das not√≠cias.

* **Modelo de Documentos Flex√≠vel:** Utiliza documentos JSON, que se adaptam perfeitamente a campos como t√≠tulo, conte√∫do e metadados vari√°veis.

* **Escalabilidade na Nuvem:** Por ser uma solu√ß√£o gerenciada, o Atlas permite f√°cil escalabilidade horizontal para lidar com grandes volumes de dados.

* **Recursos Nativos:** Oferece indexa√ß√£o e busca textual otimizadas, al√©m de integra√ß√£o direta com Apache Spark, o que √© essencial para o processamento distribu√≠do.

### Apache Spark (PySpark)

Para processar um grande volume de texto de forma eficiente, o **Apache Spark** foi a escolha ideal.

* **Processamento Distribu√≠do:** Sua arquitetura permite escalar o processamento horizontalmente, garantindo performance.

* **Ecossistema de Machine Learning:** Conta com bibliotecas integradas como **Spark MLlib** e **Spark NLP**, que facilitam o treinamento de modelos e o tratamento lingu√≠stico.

* **Execu√ß√£o Otimizada:** O uso de DataFrames e a execu√ß√£o pregui√ßosa (*lazy execution*) contribuem para uma manipula√ß√£o de dados mais r√°pida e eficiente.

### Integra√ß√£o MongoDB + Spark

A combina√ß√£o das duas tecnologias √© o grande diferencial da arquitetura. O conector oficial permite que o Spark leia e escreva dados diretamente no MongoDB, possibilitando o processamento *in-place* e reduzindo a lat√™ncia ao evitar a movimenta√ß√£o desnecess√°ria de dados.

## üìä Fonte de Dados

* **Dataset:** Fake and Real News Dataset ‚Äì Kaggle

* **Link:** [`https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset`](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)

* **Conte√∫do:** Aproximadamente 44.000 artigos, divididos em `Fake.csv` e `True.csv`.

* **Atributos:** `title`, `text`, `subject`, `date`.

## ‚öôÔ∏è Arquitetura e Fluxo do Projeto

O pipeline do projeto segue as seguintes etapas:

1. **Obten√ß√£o dos Dados:** Download dos arquivos `.csv` do Kaggle.

2. **Pr√©-processamento e Ingest√£o:** Um script em Python realiza uma limpeza inicial e insere as not√≠cias como documentos JSON no MongoDB Atlas, em *collections* separadas (`Fake` e `Real`).

3. **Leitura com Spark:** O Apache Spark se conecta ao MongoDB Atlas via conector e carrega os dados em um DataFrame distribu√≠do.

4. **Processamento com PySpark:** S√£o aplicadas transforma√ß√µes de NLP:

   * Tokeniza√ß√£o.

   * Remo√ß√£o de *stopwords*.

   * Vetoriza√ß√£o com **TF-IDF**.

5. **Treinamento do Modelo:** Algoritmos como **Naive Bayes** e **Regress√£o Log√≠stica** s√£o treinados com os dados vetorizados.

6. **An√°lise Explorat√≥ria:** Em paralelo, s√£o geradas estat√≠sticas para identificar padr√µes, como a frequ√™ncia de palavras e n-gramas.

## üìà Resultados Esperados

* **Pipeline Funcional:** Um fluxo de trabalho de ponta a ponta, demonstrando a integra√ß√£o bem-sucedida entre MongoDB Atlas e Apache Spark.

* **Modelo de Classifica√ß√£o Avaliado:** Um modelo de *machine learning* treinado e avaliado com m√©tricas de desempenho como **acur√°cia, precis√£o e recall**.

* **An√°lise Lingu√≠stica:** Gera√ß√£o de visualiza√ß√µes e estat√≠sticas, como nuvens de palavras e rankings de termos mais frequentes, que revelem os padr√µes de vocabul√°rio usados em not√≠cias falsas.
